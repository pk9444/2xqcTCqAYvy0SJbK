# VOICE CLONING AND FAKE AUDIO DETECTION 

## PHASE 1 - VOICE CLONING SYSTEM (VCS)

### CONTEXT 

The client is a tech company working in the Cyber Security industry. They developed end-to-end products and services that ensure the customers security using data driven technologies to understand whether audio and video media is authentic or fake/synthetic.

### GOALS

- Build a VCS (Voice Cloning System) that takes real/authentic audio samples, applies signal transformations, and generates their synthetic clones.
- Develop a FAD (Fake Audio Detection) using a ground truth audio dataset, that uses Machine Learning to detect whether the audio is fake or not.

### SUCCESS METRICS
- Word Error Rate (WER) for automatic evaluation of the VCS.
- Speaker classification accuracy to assess the performance of the generated audio’s target speaker.
- Classification metrics like F-1 Score via positive labels coming from the groundtruth dataset and negative labels generated by the VCS.

### DATA DESCRIPTION

For this project, we will be leveraging the following two datasets: 

- TIMIT dataset - contains speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains a total of 6300 sentences, 10 sentences spoken by each of 630 speakers from 8 major dialect regions of the United States. Our VCS will create 5 clones per speaker id.

- CommonVoice dataset - corpus of speech data read by users on the Common Voice website (https://commonvoice.mozilla.org/), and based upon text from a number of public domain sources like user submitted blog posts, old books, movies, and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems. Link - [https://commonvoice.mozilla.org/en/datasets]

### METHODOLOGY

<img width="806" height="325" alt="Apziva_p5_Phase1 drawio" src="https://github.com/user-attachments/assets/d85c8094-daa2-42e0-832b-5831e87e793b" />


## PHASE 2 - SIGNAL PROCESSING AND DATASET CONSOLIDATION

### OBJECTIVE

- Consolidate the CommonVoice audio dataset as the ground truth along with TIMIT samples from the VCS, apply signal transformations, analyze waveforms and frequency variations and save the meta data for building FAD (Fake Audio Detection) model.

### METHOLDOGY

<img width="828" height="486" alt="Apziva_p5_Phase2 drawio" src="https://github.com/user-attachments/assets/7b4711e7-a628-46e7-846a-bdcab561f505" />

### OBSERVATIONS

<img width="1654" height="922" alt="Apziva_p5_Phase2_Fake_Vs_Real_Comparison" src="https://github.com/user-attachments/assets/4cb418a9-91d8-4572-9b32-cf0648f1d600" />

| **Aspect** | **Real Audio** | **Fake Audio** |
|-------------|----------------|----------------|
| **Amplitude Pattern (Waveform)** | More natural amplitude variation with smoother transitions between speech segments. | Higher amplitude peaks and more uniform energy variations - evidence of external smoothening. |
| **Temporal Structure** | Clear silence gaps and distinct phoneme boundaries, typical of natural speech. | Less silence between segments; transitions appear more abrupt or evenly spaced. |
| **Spectrogram (Mel Bands)** | Richer harmonic structure and smoother formant transitions. | Slightly blurred or smeared harmonics, with less detailed formant definition. |
| **Overall Naturalness** | Appears authentic, with organic timing and spectral variation. | Sounds more synthetic - consistent energy and less variability in texture. |

<img width="1670" height="818" alt="Apziva_p5_Phase2_CommonVoice_GroundTruth" src="https://github.com/user-attachments/assets/9d58a113-7e4f-41a7-a74e-69059109a54b" />

- The ground truth audio sample, i.e. CommonVoice exhibits a waveform pattern that somewhat matches the waveforms of both the TIMIT real and fake audio samples. 
- The same behavior can also be noticed in its frequency variation over time in the spectrogram - it has ridges similar to the fake, but energy levels matching closer with the real.
- This actually a good sign as it denotes that the VCS has created clones very similar to the ground truth, and hence, the model would be robust to detect a deepfake audio at greater granularity.  

## PHASE 3 - FAKE AUDIO DETECTION MECHANISM

### OBJECTIVE

- Train a Convoluted Neural Network (CNN) that reads the consolidated (ground truth + real + fake) dataset, extract mel spectrograms as features, build the model, and compile the success metrics - F-1 Score, Classification Report, Word Error Rate (WER) and Speaker Classification Accuracy.

### METHODOLOGY

<img width="822" height="562" alt="Apziva_p5_Phase3_FAD_Methodology" src="https://github.com/user-attachments/assets/e138c7fc-9e27-4a15-aed2-6b84d706b3d7" />

### PERFORMANCE EVALUATION

#### CLASSIFICATION REPORT

<img width="500" height="400" alt="confmat_train" src="https://github.com/user-attachments/assets/bfffc9d7-3a48-47cc-bee2-16fcc58398b8" />
<img width="500" height="400" alt="confmat_val" src="https://github.com/user-attachments/assets/af460288-65b0-4598-b758-14150dfda105" />
<img width="500" height="400" alt="confmat_test" src="https://github.com/user-attachments/assets/738ab410-1a11-4766-ae3d-eb82c317facf" />


- The model achieves perfect or near-perfect precision, recall, and F-1 on train, validation, and test sets, suggesting extremely strong internal learning and separation between real and fake speech features.
- Validation and test performance mirrors training accuracy (≈1.00), indicating the model generalizes well within the distribution of TIMIT and CommonVoice samples used in training.
- Both "Real" and "Fake" classes achieve identical precision/recall values, showing that the model does not exhibit bias toward the majority (real) class despite the 8:3 dataset imbalance.
- The F-1 score of 1.000 confirms that the model maintains a decent balance between false positives and false negatives—critical for fake-audio detection tasks where misclassification cost is high.
- The limitation is that we have our own VCS that fakes a audio samples in a designated way, and the scope is limited to only CommonVoice and TIMIT datasets; so a more diversified, global dataset would have been even more beneifical for training the CNN.  

#### WER ANALYSIS

<img width="2400" height="1500" alt="wer_distribution" src="https://github.com/user-attachments/assets/d768a3b2-51b8-4dca-9bda-29f253ef9b5e" />

- On average, the cloned (fake) versions of the same utterances differ from their real counterparts by about 48 % of the words — indicating moderate distortion in intelligibility or phonetic clarity introduced by the VCS.
- The histogram shows most samples have WER between 0.3 and 0.6, meaning many fakes retain recognizable speech but still exhibit consistent articulation or timing mismatches detectable by ASR.
- A smaller subset exceeds WER > 1.0, corresponding to highly degraded or unnatural synthetic speech, likely where the vocoder or prosody modeling failed.
- This moderate-to-high WER range aligns with the CNN learning to flag as "fake". The model likely exploits the same degradation patterns that inflate WER — thus the two metrics reinforce one another as independent validation signals.


## PHASE 4 - DEPLOYMENT TO PRODUCTION

<img width="828" height="486" alt="Apziva_p5_Phase4 drawio" src="https://github.com/user-attachments/assets/f76e8c26-9e7a-4566-b27b-85fa0be90dcd" />

### TECHNOLOGY STACK

- Python >= 3.11.0
- HTML5
- CSS
- Plotly.js
- JupyterLab
- NGINX >= 1.28.0

### END-TO-END SOLUTION DEMO

https://github.com/user-attachments/assets/36c4091e-a032-4e2d-82c6-1b18532fcb57




