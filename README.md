# VOICE CLONING AND FAKE AUDIO DETECTION 

## PHASE 1 - VOICE CLONING SYSTEM (VCS)

### CONTEXT 

The client is a tech company working in the Cyber Security industry. They developed end-to-end products and services that ensure the customers security using data driven technologies to understand whether audio and video media is authentic or fake/synthetic.

### GOALS

- Build a VCS (Voice Cloning System) that takes real/authentic audio samples, applies signal transformations, and generates their synthetic clones.
- Develop a FAD (Fake Audio Detection) using a ground truth audio dataset, that uses Machine Learning to detect whether the audio is fake or not.

### SUCCESS METRICS
- Word Error Rate (WER) for automatic evaluation of the VCS.
- Speaker classification accuracy to assess the performance of the generated audioâ€™s target speaker.
- Classification metrics like F-1 Score via positive labels coming from the groundtruth dataset and negative labels generated by the VCS.

### DATA DESCRIPTION

For this project, we will be leveraging the following two datasets: 

- TIMIT dataset - contains speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains a total of 6300 sentences, 10 sentences spoken by each of 630 speakers from 8 major dialect regions of the United States. Our VCS will create 5 clones per speaker id.

- CommonVoice dataset - corpus of speech data read by users on the Common Voice website (https://commonvoice.mozilla.org/), and based upon text from a number of public domain sources like user submitted blog posts, old books, movies, and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems. Link - [https://commonvoice.mozilla.org/en/datasets]

### METHODOLOGY

<img width="806" height="325" alt="Apziva_p5_Phase1 drawio" src="https://github.com/user-attachments/assets/d85c8094-daa2-42e0-832b-5831e87e793b" />


## PHASE 2 - SIGNAL PROCESSING AND DATASET CONSOLIDATION

### OBJECTIVE

- Consolidate the CommonVoice audio dataset as the ground truth along with TIMIT samples from the VCS, apply signal transformations, analyze waveforms and frequency variations and save the meta data for building FAD (Fake Audio Detection) model.

### METHOLDOGY

<img width="828" height="486" alt="Apziva_p5_Phase2 drawio" src="https://github.com/user-attachments/assets/7b4711e7-a628-46e7-846a-bdcab561f505" />

### OBSERVATIONS

<img width="1654" height="922" alt="Apziva_p5_Phase2_Fake_Vs_Real_Comparison" src="https://github.com/user-attachments/assets/4cb418a9-91d8-4572-9b32-cf0648f1d600" />

| **Aspect** | **Real Audio** | **Fake Audio** |
|-------------|----------------|----------------|
| **Amplitude Pattern (Waveform)** | More natural amplitude variation with smoother transitions between speech segments. | Higher amplitude peaks and more uniform energy variations - evidence of external smoothening. |
| **Temporal Structure** | Clear silence gaps and distinct phoneme boundaries, typical of natural speech. | Less silence between segments; transitions appear more abrupt or evenly spaced. |
| **Spectrogram (Mel Bands)** | Richer harmonic structure and smoother formant transitions. | Slightly blurred or smeared harmonics, with less detailed formant definition. |
| **Overall Naturalness** | Appears authentic, with organic timing and spectral variation. | Sounds more synthetic - consistent energy and less variability in texture. |

<img width="1670" height="818" alt="Apziva_p5_Phase2_CommonVoice_GroundTruth" src="https://github.com/user-attachments/assets/9d58a113-7e4f-41a7-a74e-69059109a54b" />

- The ground truth audio sample, i.e. CommonVoice exhibits a waveform pattern that somewhat matches the waveforms of both the TIMIT real and fake audio samples. 
- The same behavior can also be noticed in its frequency variation over time in the spectrogram - it has ridges similar to the fake, but energy levels matching closer with the real.
- This actually a good sign as it denotes that the VCS has created clones very similar to the ground truth, and hence, the model would be robust to detect a deepfake audio at greater granularity.  

## PHASE 3 - FAKE AUDIO DETECTION MECHANISM

<img width="822" height="562" alt="Apziva_p5_Phase3_FAD_Methodology" src="https://github.com/user-attachments/assets/e138c7fc-9e27-4a15-aed2-6b84d706b3d7" />
